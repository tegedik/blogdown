<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on T.E.G.&#39;s Blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on T.E.G.&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sorcerer&#39;s Apprentice Approach to Statistical Analysis</title>
      <link>/hta/</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/hta/</guid>
      <description>Numbers do not produce value. The widespread availability of muscular number-crunching computers has had the untoward effect of yielding power to the sorcerer’s apprentice. Statistics and computers must support the research activity, not motivate it (Alfred 1987, 3).
 Alfred criticized neglecting the role of theory (which should guide any reseach inquiry) for the sake of “number-crunching.” This is a fair argument and repeated frequently by others. I would like to talk about a different (but related) issue using the metaphor, sorcerer’s apprentice.</description>
    </item>
    
    <item>
      <title>Schartz-Metterclume Method</title>
      <link>/sm/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/sm/</guid>
      <description>Now I&amp;#39;m down an internet rabbit hole thinking about how #polisci journals have published some seriously random crap, including joke articles by fake authors that cite each other. The humor of all of this is lost on me. A few highlights:
&amp;mdash; Thomas J. Leeper (@thosjleeper) April 21, 2018  I just saw the above tweet by Thomas J. Leeper,1 and indeed the rabbit hole is deep. One article on the list is “The Schartz-Metterclume Method”2 by Dagobert D.</description>
    </item>
    
    <item>
      <title>Recoding/Labeling Variables</title>
      <link>/rec/</link>
      <pubDate>Sat, 24 Mar 2018 00:12:00 +0000</pubDate>
      
      <guid>/rec/</guid>
      <description>This is a simple topic. And there are various ways of recoding and (re)labeling variables in R. But rec function from sjmisc package (Lüdecke 2018) works best for me. Give it a try:
library(tidyverse) library(strengejacke) mtcars %&amp;gt;% select(disp, mpg, cyl) %&amp;gt;% rec(cyl,rec= &amp;quot;4=1 [low]; 6=2 [mid]; 8=3 [high]&amp;quot;) %&amp;gt;% to_label(cyl_r) ## # A tibble: 32 x 4 ## disp mpg cyl cyl_r ## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; ## 1 160. 21.</description>
    </item>
    
    <item>
      <title>Try Zelig: Everyone&#39;s Statistical Software</title>
      <link>/tryzelig/</link>
      <pubDate>Tue, 13 Mar 2018 20:01:00 +0000</pubDate>
      
      <guid>/tryzelig/</guid>
      <description>It is quite common to hear complaints about the steep learning curve of R. And it is true that it would take time to grasp its versatility (if it is possible at all). Nevertheless, there are many packages that give the opportunity of experiencing the power of R for beginners. One notable example is Zelig package (Choirat et al. 2017). The online documentation provides examples for a wide range of models.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation: Finding the Top of a Hill</title>
      <link>/climbing/</link>
      <pubDate>Tue, 06 Feb 2018 18:34:54 +0000</pubDate>
      
      <guid>/climbing/</guid>
      <description>I think one of the most intuitive descriptions of the maximum likelihood estimation (especially for the beginners) can be found in Long and Freese (2014):
 For all but the simplest models, the only way to find the maximum likelihood function is by numerical methods.1 Numerical methods are the mathematical equivalent of how you would find the top of a hill if you were blindfolded and knew only the slope of the hill at the spot where you are standing and how the slope at that spot is changing which you could figure out by poking your foot in each direction.</description>
    </item>
    
    <item>
      <title>Binomial GLM Predicted Probabilities</title>
      <link>/binomglm/</link>
      <pubDate>Sun, 04 Feb 2018 17:55:34 +0000</pubDate>
      
      <guid>/binomglm/</guid>
      <description>Here is an example of binomial glm (e.g., logistic regression):
df &amp;lt;- car::Mroz # U.S. Women&amp;#39;s Labor-Force Participation data glmfit &amp;lt;- glm(lfp ~ k5 + age + inc + lwg, df, family=binomial) summary(glmfit) ## ## Call: ## glm(formula = lfp ~ k5 + age + inc + lwg, family = binomial, ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7987 -1.1326 0.6516 0.9605 2.</description>
    </item>
    
    <item>
      <title>Why Not Transform?</title>
      <link>/why-not-transform/</link>
      <pubDate>Sun, 04 Feb 2018 17:54:34 +0000</pubDate>
      
      <guid>/why-not-transform/</guid>
      <description>Why do we want to deal with the trouble of estimating generalized linear models (GLM) when it is possible to transform the response variable and estimate models using linear regression? It is worth to mention two answers (Agresti 2015):
It is hard to find a transformation which provides both normality and constant variance (see assumptions of OLS).
 Interpretability. To illustrate, first see the formula of the transformed-data approach
  \[E[g(y_i)]= \beta_0+\beta_1x_1 + \ldots+\beta_kx_k\]</description>
    </item>
    
    <item>
      <title>Lorem Ipsum</title>
      <link>/lorem-ipsum/</link>
      <pubDate>Sun, 04 Feb 2018 13:59:49 +0000</pubDate>
      
      <guid>/lorem-ipsum/</guid>
      <description>The passage from Lorem Ipsum is here, because it deserves a homage:
 What I find remarkable is that this text has been the industry&amp;rsquo;s standard dummy text ever since some printed in the 1500s took a galley of type and scrambled it to make a type specimen book; it has survived not only four centuries of letter-by-letter resetting but even the leap into electronic typesetting, essentially unchanged except for an occasional &amp;lsquo;ing&amp;rsquo; or &amp;lsquo;y&amp;rsquo; thrown in.</description>
    </item>
    
  </channel>
</rss>